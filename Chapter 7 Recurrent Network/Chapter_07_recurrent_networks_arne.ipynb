{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Chapter_07_recurrent_networks_arne.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Qi_ztJjRkoFD","colab_type":"text"},"source":["##### Author contributions\n","Please fill out for each of the following parts who contributed to what:\n","- Conceived ideas: Everybody\n","- Performed math exercises: Everybody\n","- Performed programming exercises: Everybody\n","- Contributed to the overall final assignment: Everybody\n","\n","Everybody did the assignment themselves and we voted afterwards for the one version to submit."]},{"cell_type":"markdown","metadata":{"id":"D6F94N5QkoFE","colab_type":"text"},"source":["# Chapter 7\n","## Recurrent neural networks\n","\n","\n","    Hand-in bug-free (try \"Kernel\" > \"Restart & Run All\") and including all (textual as well as figural) output via Brightspace before the deadline (see Brightspace).\n","\n","Learning goals:\n","1. Get familiar with recurrent hidden units\n","1. Implement a simple RNN (Elman network) in PyTorch\n","1. Implement an LSTM-based neural network in PyTorch"]},{"cell_type":"code","metadata":{"id":"Z9bDnZ6PkoFF","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_7qSZaaXkoFI","colab_type":"text"},"source":["### Exercise 1  (1 point)"]},{"cell_type":"markdown","metadata":{"id":"Ij6rZRK5koFJ","colab_type":"text"},"source":["Consider a recurrent neural network with one input unit $x$, one sigmoid recurrent hidden unit $h$, and one linear output unit $y$. The values of $x$ are given for 3 time points in `x_t`. As this is a very small RNN, $W^i$, $W^h$ and $W^o$ are given as the scalar values `w_i`, `w_h` and `w_o` respectively. The hidden unit has an added bias `h_bias`. The hidden unit state is initialized with `0.0`. The only 'value-manipulating' activation function in this network is the sigmoid activation $\\sigma(\\cdot)$ on the hidden unit. \n","\n","1. Write down the forward pass of this network for a specific time point $t$. \n","1. What is the value of the hidden state $h$ after processing the last input `x_t[2]`? \n","1. What is the output `y` of the network after processing the last input `x_t[2]`? \n","\n","\\begin{eqnarray*}\n","h_t &=& \\sigma(W^i*x(t) + W^h*h(t-1) + b_h) \\\\ \n","y_t &=&  W^o*h(t)\\\\\n","\\end{eqnarray*}\n","\n","\n","For 1.2 and 1.3, you can either compute the solution by hand (show clearly how you arrived there, 3 decimal points) or write code to find the answer. "]},{"cell_type":"code","metadata":{"id":"JdQH5a3ykoFJ","colab_type":"code","colab":{}},"source":["# inputs over times 0, 1, 2:\n","x_t = [9.0, 4.0, -2.0]\n","\n","# weights and bias terms: \n","w_i = 0.5\n","w_h = -1.0\n","w_o = -0.7\n","h_bias = -1.0\n","y_bias = 0.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aS6GdWdqkoFM","colab_type":"text"},"source":["### Solution 1"]},{"cell_type":"code","metadata":{"id":"pNRYY73t8SIB","colab_type":"code","colab":{}},"source":["def sigmoid(A):\n","    \"\"\"\n","    Computes the sigmoid activation function. Taken from Chapter04\n","    INPUT:\n","        A = [H N] activity matrix of H units for N examples\n","    OUTPUT\n","        Y = [H N] output matrix of H units for N examples\n","    \"\"\"\n","    Y = 1 / (1 + np.exp(-A))\n","    return Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fSaZ77B5koFM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1590077154475,"user_tz":-120,"elapsed":5643,"user":{"displayName":"AD","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjx_I26ZGCa4R1NaUrO4rg8RHBTjADOtriSJZ03_A=s64","userId":"12357515783115079539"}},"outputId":"fa8de99a-0453-4118-93d4-88ea8369f186"},"source":["h = np.zeros((3))\n","\n","h[0] = sigmoid(w_i * x_t[0] + h_bias)\n","h[1] = sigmoid(w_i * x_t[1] + w_h * h[0] + h_bias)\n","h[2] = sigmoid(w_i * x_t[2] + w_h * h[1] + h_bias)\n","\n","y = w_o * h[2] + y_bias\n","\n","### 1.2\n","print(\"h2: \",h[2])\n","\n","### 1.3\n","print(\"y: \",y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["h2:  0.07534608655539546\n","y:  -0.05274226058877682\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U5OmShUAkoFP","colab_type":"text"},"source":["### Code introduction\n","\n","We will apply two recurrent neural networks to learn a dynamic variant of the *adding problem*. First, run the next cell and inspect the output. \n","\n","There is a stream of inputs to the network, two at each time step. The first input unit will receive a series of decimal numbers in the interval $[-1,1]$. The second input unit will receive the numbers $0$, $-1$, or $1$. The target is the sum of the preceding two decimal numbers that came together with the number $1$ (called the marker, `x` in the generated output), and it should be produced whenever a marker has been seen. In the beginning until two of these markers have been seen, the output will stay 0. \n","\n","\n","Below you will find two functions: \n","1. `create_addition_data`: Generates sequential training data sets `X` and `T` for the dynamic *adding problem*, returns numpy array.\n","1. `MyDataset`: a custom PyTorch dataset that makes sure dimensions are as PyTorch likes them, and can return individual samples the way PyTorch wants them.\n","\n","Note, the data are represented in a dictionary called `data`. To access the training, validation, and testing data, you can call `data[\"train\"]`, `data[\"valid]`, and `data[\"test\"]` respectively."]},{"cell_type":"code","metadata":{"id":"ktstRGvPkoFP","colab_type":"code","colab":{}},"source":["def create_addition_data(n_samples=3000):\n","    # This is a dynamic variant of the adding problem. \n","    \n","    # Random numbers in [-1.0,1.0]): \n","    X1 = np.random.uniform(low=-1.0, high=1.0, size=(n_samples,) )   \n","    \n","    # Random markers [-1.0, 0.0, 1.0] (1.0 marks the numbers that should be added):\n","    X2 = np.random.choice([-1.0, 0.0, 1.0], size=(n_samples,), p=[0.25, 0.25, 0.5])\n","    \n","    # Combine\n","    X = np.vstack((X1, X2)).T.astype(\"float32\")\n","\n","    # Create targets\n","    T = np.zeros((n_samples, 1)).astype(\"float32\")\n","\n","    # Get indices of 1.0\n","    markers = np.nonzero(X2 == 1.0)[0]\n","    \n","    # Generate data\n","    mem = X1[markers[0]]\n","    for mi, marker in enumerate(markers[1:]):\n","        T[marker] = mem + X1[marker]\n","        mem = X1[marker]\n","                \n","    return X, T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O1hB0KackoFR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1590077154479,"user_tz":-120,"elapsed":5626,"user":{"displayName":"AD","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjx_I26ZGCa4R1NaUrO4rg8RHBTjADOtriSJZ03_A=s64","userId":"12357515783115079539"}},"outputId":"98eba3a7-aa39-429d-9b2b-2dbfadc8f2e0"},"source":["# Long as the markers x are sparse\n","X, T = create_addition_data(n_samples=100)\n","\n","# Print some data\n","print(\"Data for the adding problem (x marks 1.0):\")\n","for t in range(X.shape[0]):\n","    print(\"Time: {:03d} \\t x: ({:+.3f} , {}) \\t t: {:+.3f} \".format(\n","        t, X[t,0], 'x' if X[t,1] == 1.0 else ' ', T[t,0]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Data for the adding problem (x marks 1.0):\n","Time: 000 \t x: (-0.189 , x) \t t: +0.000 \n","Time: 001 \t x: (-0.906 , x) \t t: -1.096 \n","Time: 002 \t x: (-0.863 , x) \t t: -1.769 \n","Time: 003 \t x: (-0.278 ,  ) \t t: +0.000 \n","Time: 004 \t x: (-0.109 , x) \t t: -0.972 \n","Time: 005 \t x: (+0.656 , x) \t t: +0.547 \n","Time: 006 \t x: (+0.475 ,  ) \t t: +0.000 \n","Time: 007 \t x: (+0.478 ,  ) \t t: +0.000 \n","Time: 008 \t x: (+0.981 , x) \t t: +1.637 \n","Time: 009 \t x: (+0.947 ,  ) \t t: +0.000 \n","Time: 010 \t x: (-0.349 ,  ) \t t: +0.000 \n","Time: 011 \t x: (-0.414 ,  ) \t t: +0.000 \n","Time: 012 \t x: (+0.955 ,  ) \t t: +0.000 \n","Time: 013 \t x: (+0.877 , x) \t t: +1.858 \n","Time: 014 \t x: (+0.580 ,  ) \t t: +0.000 \n","Time: 015 \t x: (+0.370 , x) \t t: +1.247 \n","Time: 016 \t x: (+0.192 , x) \t t: +0.563 \n","Time: 017 \t x: (-0.501 ,  ) \t t: +0.000 \n","Time: 018 \t x: (-0.074 ,  ) \t t: +0.000 \n","Time: 019 \t x: (-0.948 , x) \t t: -0.756 \n","Time: 020 \t x: (-0.791 ,  ) \t t: +0.000 \n","Time: 021 \t x: (+0.261 , x) \t t: -0.687 \n","Time: 022 \t x: (-0.157 , x) \t t: +0.105 \n","Time: 023 \t x: (-0.396 ,  ) \t t: +0.000 \n","Time: 024 \t x: (-0.576 , x) \t t: -0.733 \n","Time: 025 \t x: (+0.967 ,  ) \t t: +0.000 \n","Time: 026 \t x: (+0.620 ,  ) \t t: +0.000 \n","Time: 027 \t x: (-0.673 , x) \t t: -1.249 \n","Time: 028 \t x: (+0.629 , x) \t t: -0.044 \n","Time: 029 \t x: (+0.309 ,  ) \t t: +0.000 \n","Time: 030 \t x: (-0.790 , x) \t t: -0.161 \n","Time: 031 \t x: (+0.877 ,  ) \t t: +0.000 \n","Time: 032 \t x: (-0.300 , x) \t t: -1.090 \n","Time: 033 \t x: (-0.591 ,  ) \t t: +0.000 \n","Time: 034 \t x: (+0.922 , x) \t t: +0.621 \n","Time: 035 \t x: (-0.669 , x) \t t: +0.253 \n","Time: 036 \t x: (+0.839 , x) \t t: +0.170 \n","Time: 037 \t x: (-0.381 , x) \t t: +0.459 \n","Time: 038 \t x: (+0.815 , x) \t t: +0.435 \n","Time: 039 \t x: (-0.600 ,  ) \t t: +0.000 \n","Time: 040 \t x: (+0.920 , x) \t t: +1.735 \n","Time: 041 \t x: (-0.440 , x) \t t: +0.480 \n","Time: 042 \t x: (+0.234 ,  ) \t t: +0.000 \n","Time: 043 \t x: (+0.004 , x) \t t: -0.436 \n","Time: 044 \t x: (-0.053 , x) \t t: -0.049 \n","Time: 045 \t x: (+0.879 ,  ) \t t: +0.000 \n","Time: 046 \t x: (+0.527 ,  ) \t t: +0.000 \n","Time: 047 \t x: (+0.140 , x) \t t: +0.087 \n","Time: 048 \t x: (-0.469 , x) \t t: -0.329 \n","Time: 049 \t x: (+0.949 , x) \t t: +0.480 \n","Time: 050 \t x: (-0.235 ,  ) \t t: +0.000 \n","Time: 051 \t x: (-0.131 , x) \t t: +0.818 \n","Time: 052 \t x: (-0.584 , x) \t t: -0.715 \n","Time: 053 \t x: (+0.340 , x) \t t: -0.244 \n","Time: 054 \t x: (-0.156 ,  ) \t t: +0.000 \n","Time: 055 \t x: (+0.820 ,  ) \t t: +0.000 \n","Time: 056 \t x: (-0.337 , x) \t t: +0.002 \n","Time: 057 \t x: (+0.189 , x) \t t: -0.148 \n","Time: 058 \t x: (-0.981 ,  ) \t t: +0.000 \n","Time: 059 \t x: (-0.639 ,  ) \t t: +0.000 \n","Time: 060 \t x: (-0.411 , x) \t t: -0.222 \n","Time: 061 \t x: (+0.297 , x) \t t: -0.114 \n","Time: 062 \t x: (+0.166 , x) \t t: +0.463 \n","Time: 063 \t x: (-0.149 ,  ) \t t: +0.000 \n","Time: 064 \t x: (-0.399 ,  ) \t t: +0.000 \n","Time: 065 \t x: (+0.674 ,  ) \t t: +0.000 \n","Time: 066 \t x: (-0.716 , x) \t t: -0.550 \n","Time: 067 \t x: (+0.400 , x) \t t: -0.316 \n","Time: 068 \t x: (-0.411 ,  ) \t t: +0.000 \n","Time: 069 \t x: (-0.635 , x) \t t: -0.235 \n","Time: 070 \t x: (-0.740 ,  ) \t t: +0.000 \n","Time: 071 \t x: (+0.504 ,  ) \t t: +0.000 \n","Time: 072 \t x: (+0.262 , x) \t t: -0.373 \n","Time: 073 \t x: (+0.546 ,  ) \t t: +0.000 \n","Time: 074 \t x: (-0.440 , x) \t t: -0.177 \n","Time: 075 \t x: (+0.365 , x) \t t: -0.075 \n","Time: 076 \t x: (-0.079 ,  ) \t t: +0.000 \n","Time: 077 \t x: (+0.893 ,  ) \t t: +0.000 \n","Time: 078 \t x: (+0.585 ,  ) \t t: +0.000 \n","Time: 079 \t x: (+0.369 , x) \t t: +0.734 \n","Time: 080 \t x: (+0.172 , x) \t t: +0.541 \n","Time: 081 \t x: (-0.004 , x) \t t: +0.168 \n","Time: 082 \t x: (-0.059 ,  ) \t t: +0.000 \n","Time: 083 \t x: (+0.930 , x) \t t: +0.926 \n","Time: 084 \t x: (+0.300 ,  ) \t t: +0.000 \n","Time: 085 \t x: (+0.931 ,  ) \t t: +0.000 \n","Time: 086 \t x: (-0.387 , x) \t t: +0.543 \n","Time: 087 \t x: (-0.378 , x) \t t: -0.765 \n","Time: 088 \t x: (-0.321 , x) \t t: -0.699 \n","Time: 089 \t x: (-0.697 , x) \t t: -1.018 \n","Time: 090 \t x: (-0.890 , x) \t t: -1.587 \n","Time: 091 \t x: (-0.957 ,  ) \t t: +0.000 \n","Time: 092 \t x: (+0.681 ,  ) \t t: +0.000 \n","Time: 093 \t x: (+0.090 ,  ) \t t: +0.000 \n","Time: 094 \t x: (-0.899 , x) \t t: -1.788 \n","Time: 095 \t x: (-0.902 ,  ) \t t: +0.000 \n","Time: 096 \t x: (+0.409 , x) \t t: -0.490 \n","Time: 097 \t x: (-0.799 , x) \t t: -0.391 \n","Time: 098 \t x: (+0.324 ,  ) \t t: +0.000 \n","Time: 099 \t x: (-0.330 , x) \t t: -1.129 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"frg1RSw_koFT","colab_type":"code","colab":{}},"source":["# Make PyTorch dataset\n","class MyDataset(torch.utils.data.Dataset):\n","    \n","    def __init__(self, X, T):\n","        self.X = torch.from_numpy(X).type(torch.FloatTensor) # [n_examples, n_samples, n_features]\n","        self.T = torch.from_numpy(T).type(torch.FloatTensor) # [n_examples, n_samples]\n","        \n","    def __getitem__(self, index):\n","        return self.X[index, :, :], self.T[index]\n","    \n","    def __len__(self):\n","        return self.X.size()[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rJ0OSXk9koFW","colab_type":"code","colab":{}},"source":["n_examples = 9\n","n_samples = 3000\n","data = {}\n","\n","# Define training data\n","X = np.zeros((n_examples, n_samples, 2))\n","T = np.zeros((n_examples, n_samples, 1))\n","for i_example in range(n_examples):\n","    X[i_example, :, :], T[i_example, :] = create_addition_data(n_samples)\n","data[\"train\"] = torch.utils.data.DataLoader(MyDataset(X, T), batch_size=3)\n","\n","# Define validation data\n","X = np.zeros((n_examples, n_samples, 2))\n","T = np.zeros((n_examples, n_samples, 1))\n","for i_example in range(n_examples):\n","    X[i_example, :, :], T[i_example, :] = create_addition_data(n_samples)\n","data[\"valid\"] = torch.utils.data.DataLoader(MyDataset(X, T), batch_size=3)\n","\n","# Define test data\n","X = np.zeros((n_examples, n_samples, 2))\n","T = np.zeros((n_examples, n_samples, 1))\n","for i_example in range(n_examples):\n","    X[i_example, :, :], T[i_example, :] = create_addition_data(n_samples)\n","data[\"test\"] = torch.utils.data.DataLoader(MyDataset(X, T), batch_size=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ro_pbo-1koFY","colab_type":"text"},"source":["### Exercise 2: training a network  (0.5 points)\n","\n","We neede a function to train a `model`. This function `train_model(model, data, optimizer, criterion, n_epochs)` should do the following: \n","\n","1. Loop `n_epochs` times over the dataset, and loop over minibatches\n","1. Train the model on the training data and save the loss per epoch\n","1. Validate the model on the validation data and save the loss per epoch\n","1. The function should return the trained model and the losses\n","\n","Note: this function is quite similar again as the function your wrote wor the MLP and CNN. The only difference is that we do not need to compute an accuracy, as we are performing regression here."]},{"cell_type":"markdown","metadata":{"id":"U_9GpCdBkoFZ","colab_type":"text"},"source":["### Solution 2"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pajr1raDtDvp","colab":{}},"source":["def train_model(model, data, optimizer, criterion, n_epochs):\n","  train_loss = np.zeros((n_epochs), dtype=np.float32)\n","  valid_loss = np.zeros((n_epochs), dtype=np.float32)\n","  start = 0\n","\n","  for epoch in range(n_epochs):\n","    train_total = 0\n","    val_total = 0\n","    start = time.time()\n","    try:\n","      torch.nn.init.zeros_(model.hr.weight)\n","    except:\n","      pass\n","\n","\n","    #validation loop\n","    with torch.no_grad():\n","      for val_idx, minibatch in enumerate(data[\"valid\"], 0):\n","        \n","        val_images, val_labels = minibatch[0].to(device), minibatch[1].to(device)\n","        val_outputs = model(val_images)\n","        val_loss = criterion(val_outputs, val_labels)\n","        valid_loss[epoch]+=val_loss.item()\n","\n","    #training loop\n","    with torch.enable_grad():\n","      for train_idx, minibatch in enumerate(data[\"train\"], 0):\n","        model.zero_grad()\n","        images, labels = minibatch[0].to(device), minibatch[1].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        train_loss[epoch]+=loss.item()\n","      \n","    train_loss[epoch] = train_loss[epoch] / train_idx\n","    valid_loss[epoch] = valid_loss[epoch] / val_idx\n","\n","    print(\"Epoch: \", epoch, \" t_loss: \", train_loss[epoch], \" v_loss: \", valid_loss[epoch], \" t: \", time.time()-start)\n","    start = 0\n","\n","  return model, train_loss, valid_loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8US7dRU7koFb","colab_type":"text"},"source":["### Exercise 3: Testing a network  (1.5 points)\n","\n","We neede a function to test a trained `model`. This function `test_model(model, data)` should do the following: \n","\n","1. Let `model` predict outputs on the testing data. For this, iterate through test data `data[\"test\"]` and pass each sample through `model`. \n","1. Save the model output as well as the target output\n","1. The function should return the predicted and target outputs"]},{"cell_type":"markdown","metadata":{"id":"2tl2XzoqkoFb","colab_type":"text"},"source":["### Solution 3"]},{"cell_type":"code","metadata":{"id":"l4q0o3mYkoFc","colab_type":"code","colab":{}},"source":["def test_model(model, data):\n","  predicted = []\n","  targets = []\n","  with torch.no_grad():\n","    for test_idx, minibatch in enumerate(data[\"test\"], 0):\n","      test_image, test_label = minibatch[0].to(device), minibatch[1].to(device)\n","      test_outputs = model(test_image)\n","      predicted.append(test_outputs)\n","      targets.append(test_label)\n","  return predicted, targets"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sOhK9rWIkoFe","colab_type":"text"},"source":["### Exercise 4: Simple RNN  (3 points)\n","\n","We first implement a simple recurrent architecture (a simple [Elman network](http://mnemstudio.org/neural-networks-elman.htm)). \n","\n","1. First implement the linear layers `l1` and `l2`. They should lead from `n_input` input units over `n_hidden` hidden units to `n_out` output units.\n","1. Add a recurrent linear weight layer `hr`. These are weights that self-connect to the hidden units. The input will be the values of the `n_hidden` hidden units, and they should project back to the `n_hidden` hidden units. \n","1. A forward pass will update the hidden state with the inputs and the recurrent layer weights, and produce the output from the hidden unit. Specifically you should do the following: \n","    2. If we are at the first time point, the hidden state should be set to the input passed through `l1` and `tanh` activations.\n","    2. If the hidden state has information from previous time points: a) Pass the input through `l1`. b) Pass the hidden state through the recurrent weight layer `hr`. c) The sum of a) and b) should be passed through the `tanh` activation. d) The result should be the new hidden state (used for the next time point).\n","    2. Finally pass the hidden state through layer `l2`. This produces the output `y` for that time point.\n","1. The forward pass will receive data `x` with shape [batch_size, time_points, features]. So within the forward pass, you will have to loop over time points, performing the steps as descibed above. The output of the forward pass is then output `y` with shape [batch_size, time_points, 1].\n","\n","Note: this exercise could also be done with nn.RNN(). However, we want you to understand what a RNN is doing, so we want you to use nn.Linear instead."]},{"cell_type":"markdown","metadata":{"id":"30SoobCbkoFf","colab_type":"text"},"source":["### Solution 4"]},{"cell_type":"code","metadata":{"id":"YxmGVTCKkoFf","colab_type":"code","colab":{}},"source":["from collections import OrderedDict\n","\n","class Elman(nn.Module):\n","    def __init__(self, n_input, n_hidden, n_out):\n","        super(Elman, self).__init__()\n","        self.l1 = nn.Linear(n_input, n_hidden)\n","        self.tanh = nn.Tanh()\n","        self.hr = nn.Linear(n_hidden, n_hidden)\n","        self.l2 = nn.Linear(n_hidden,n_out)\n","\n","    def forward(self, x):\n","      y = torch.empty((x.shape[0], x.shape[1], 1), requires_grad=False).to(device)\n","\n","      for time in range(x.shape[1]):\n","        if time == 0:\n","          hidden_states = self.tanh(self.l1(x[:,time]))\n","        else:\n","          a = self.l1(x[:,time])\n","          b = self.hr(hidden_states)\n","          hidden_states = self.tanh(a+b)\n","        y[:, time] = self.l2(hidden_states)\n","      return y"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9amH9CoikoFh","colab_type":"text"},"source":["### Exercise 5: Setup and run (1 point)\n","\n","Try your simple `RNN` with the dynamic addition task. \n","\n","1. Define the model. `RNN` should have **2 hidden units**.\n","1. Define the loss as the Mean Squared Error loss, and use an Adam optimizer.\n","1. Train your model for several epochs on the data with `train_model`.\n","1. Plot the train and validation losses. \n","1. Test the trained model with `test_model` \n","1. Plot at least one target time series together with the predicted time series\n","\n","Based on the losses and predictions, what would your conclusion be? Did the simple RNN learn the task? "]},{"cell_type":"markdown","metadata":{"id":"6F8NxHP9koFi","colab_type":"text"},"source":["### Solution 5"]},{"cell_type":"code","metadata":{"id":"tsBABi5jkoFi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1589808177430,"user_tz":-120,"elapsed":2093055,"user":{"displayName":"AD","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjx_I26ZGCa4R1NaUrO4rg8RHBTjADOtriSJZ03_A=s64","userId":"12357515783115079539"}},"outputId":"ae1b9fdb-b673-47ea-c66c-8e46151e4300"},"source":["elman = Elman(2,2,1)\n","torch.nn.init.zeros_(elman.l1.weight)\n","torch.nn.init.zeros_(elman.l2.weight)\n","\n","criterion = nn.MSELoss(reduction='sum',)\n","optimizer = optim.Adam(elman.parameters(), lr=0.005, amsgrad=False)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = \"cpu\"\n","elman.to(device)\n","\n","model, train_loss, val_loss = train_model(elman, data, optimizer, criterion, n_epochs=1000)\n","\n","# Plot losses\n","plt.plot(range(1,train_loss.shape[0]+1),train_loss)\n","plt.plot(range(1,val_loss.shape[0]+1),val_loss)\n","plt.legend(('train_loss', 'val_loss'), loc='upper right')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.title('Loss network')\n","plt.show()\n","\n","predicted, targets = test_model(model, data)\n","\n","plt.figure(figsize=([20,10]))\n","plt.plot(range(0,3000),targets[1][0])\n","plt.plot(range(0,3000),predicted[1][0])\n","plt.legend(('target', 'prediction'), loc='upper right')\n","plt.ylabel('Value')\n","plt.xlabel('Time')\n","plt.title('Time Series')\n","plt.show()\n","\n","plt.figure(figsize=([20,10]))\n","plt.plot(range(0,3000),targets[2][0])\n","plt.plot(range(0,3000),predicted[2][0])\n","plt.legend(('target', 'prediction'), loc='upper right')\n","plt.ylabel('Value')\n","plt.xlabel('Time')\n","plt.title('Time Series')\n","plt.show()\n","\n","plt.figure(figsize=([20,10]))\n","plt.plot(range(0,3000),targets[8][0])\n","plt.plot(range(0,3000),predicted[8][0])\n","plt.legend(('target', 'prediction'), loc='upper right')\n","plt.ylabel('Value')\n","plt.xlabel('Time')\n","plt.title('Time Series')\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch:  0  t_loss:  6233.7686  v_loss:  6294.411  t:  4.249569416046143\n","Epoch:  1  t_loss:  5963.0186  v_loss:  6024.122  t:  4.0452611446380615\n","Epoch:  2  t_loss:  5702.171  v_loss:  5762.5464  t:  4.038117408752441\n","Epoch:  3  t_loss:  5455.1963  v_loss:  5513.2905  t:  3.8663175106048584\n","Epoch:  4  t_loss:  5227.0137  v_loss:  5281.046  t:  3.914708137512207\n","Epoch:  5  t_loss:  5022.594  v_loss:  5070.735  t:  3.9353415966033936\n","Epoch:  6  t_loss:  4846.313  v_loss:  4886.931  t:  3.9162347316741943\n","Epoch:  7  t_loss:  4701.1553  v_loss:  4733.126  t:  3.951385021209717\n","Epoch:  8  t_loss:  4587.872  v_loss:  4610.8804  t:  3.909842014312744\n","Epoch:  9  t_loss:  4504.5234  v_loss:  4519.1836  t:  3.9241716861724854\n","Epoch:  10  t_loss:  4446.7236  v_loss:  4454.493  t:  3.9942243099212646\n","Epoch:  11  t_loss:  4408.2764  v_loss:  4411.2812  t:  3.91511869430542\n","Epoch:  12  t_loss:  4381.9688  v_loss:  4382.68  t:  3.949875831604004\n","Epoch:  13  t_loss:  4360.709  v_loss:  4361.4277  t:  4.02503776550293\n","Epoch:  14  t_loss:  4338.7695  v_loss:  4341.1143  t:  3.974074363708496\n","Epoch:  15  t_loss:  4312.612  v_loss:  4317.2544  t:  3.9014368057250977\n","Epoch:  16  t_loss:  4280.921  v_loss:  4287.679  t:  3.9361000061035156\n","Epoch:  17  t_loss:  4244.0225  v_loss:  4252.186  t:  3.933070182800293\n","Epoch:  18  t_loss:  4203.066  v_loss:  4211.7637  t:  3.9024641513824463\n","Epoch:  19  t_loss:  4159.338  v_loss:  4167.8174  t:  3.989013433456421\n","Epoch:  20  t_loss:  4113.8735  v_loss:  4121.6553  t:  3.8853094577789307\n","Epoch:  21  t_loss:  4067.3398  v_loss:  4074.2646  t:  3.856688976287842\n","Epoch:  22  t_loss:  4020.101  v_loss:  4026.316  t:  3.904273509979248\n","Epoch:  23  t_loss:  3972.3467  v_loss:  3978.2417  t:  3.9112355709075928\n","Epoch:  24  t_loss:  3924.2007  v_loss:  3930.298  t:  3.9099364280700684\n","Epoch:  25  t_loss:  3875.7876  v_loss:  3882.6094  t:  3.847672700881958\n","Epoch:  26  t_loss:  3827.2876  v_loss:  3835.2188  t:  3.92463755607605\n","Epoch:  27  t_loss:  3778.997  v_loss:  3788.1987  t:  3.894967794418335\n","Epoch:  28  t_loss:  3731.3975  v_loss:  3741.793  t:  3.8918797969818115\n","Epoch:  29  t_loss:  3685.1328  v_loss:  3696.4712  t:  3.912722587585449\n","Epoch:  30  t_loss:  3640.8755  v_loss:  3652.8481  t:  3.9012157917022705\n","Epoch:  31  t_loss:  3599.1494  v_loss:  3611.4897  t:  3.9715018272399902\n","Epoch:  32  t_loss:  3560.2478  v_loss:  3572.769  t:  3.9114887714385986\n","Epoch:  33  t_loss:  3524.293  v_loss:  3536.871  t:  3.9916038513183594\n","Epoch:  34  t_loss:  3491.371  v_loss:  3503.9038  t:  3.897595167160034\n","Epoch:  35  t_loss:  3461.6064  v_loss:  3473.9946  t:  3.9005372524261475\n","Epoch:  36  t_loss:  3435.1262  v_loss:  3447.287  t:  3.853930950164795\n","Epoch:  37  t_loss:  3411.9702  v_loss:  3423.852  t:  3.9055914878845215\n","Epoch:  38  t_loss:  3392.0396  v_loss:  3403.6216  t:  3.868140697479248\n","Epoch:  39  t_loss:  3375.1084  v_loss:  3386.3884  t:  3.9679253101348877\n","Epoch:  40  t_loss:  3360.858  v_loss:  3371.8506  t:  3.8930838108062744\n","Epoch:  41  t_loss:  3348.9023  v_loss:  3359.6443  t:  3.8798885345458984\n","Epoch:  42  t_loss:  3338.7957  v_loss:  3349.351  t:  3.882077932357788\n","Epoch:  43  t_loss:  3330.047  v_loss:  3340.511  t:  3.9192023277282715\n","Epoch:  44  t_loss:  3322.164  v_loss:  3332.6436  t:  3.9680488109588623\n","Epoch:  45  t_loss:  3314.6821  v_loss:  3325.2817  t:  3.881664991378784\n","Epoch:  46  t_loss:  3307.1746  v_loss:  3317.9895  t:  3.842317819595337\n","Epoch:  47  t_loss:  3299.2583  v_loss:  3310.3643  t:  3.883791923522949\n","Epoch:  48  t_loss:  3290.585  v_loss:  3302.0388  t:  3.8391411304473877\n","Epoch:  49  t_loss:  3280.8594  v_loss:  3292.6914  t:  4.015900135040283\n","Epoch:  50  t_loss:  3269.8352  v_loss:  3282.0566  t:  3.94058895111084\n","Epoch:  51  t_loss:  3257.331  v_loss:  3269.935  t:  3.8115811347961426\n","Epoch:  52  t_loss:  3243.232  v_loss:  3256.1997  t:  3.8737800121307373\n","Epoch:  53  t_loss:  3227.4937  v_loss:  3240.7961  t:  3.9747540950775146\n","Epoch:  54  t_loss:  3210.1362  v_loss:  3223.7422  t:  3.876690626144409\n","Epoch:  55  t_loss:  3191.2358  v_loss:  3205.1113  t:  3.9574472904205322\n","Epoch:  56  t_loss:  3170.9001  v_loss:  3185.021  t:  3.8379769325256348\n","Epoch:  57  t_loss:  3149.2585  v_loss:  3163.6084  t:  3.9269490242004395\n","Epoch:  58  t_loss:  3126.4368  v_loss:  3141.0156  t:  3.8278095722198486\n","Epoch:  59  t_loss:  3102.5547  v_loss:  3117.3728  t:  3.81878924369812\n","Epoch:  60  t_loss:  3077.7168  v_loss:  3092.7964  t:  3.8258912563323975\n","Epoch:  61  t_loss:  3052.0195  v_loss:  3067.3894  t:  3.9041800498962402\n","Epoch:  62  t_loss:  3025.5552  v_loss:  3041.2441  t:  3.9833624362945557\n","Epoch:  63  t_loss:  2998.4219  v_loss:  3014.4531  t:  3.8932719230651855\n","Epoch:  64  t_loss:  2970.731  v_loss:  2987.1191  t:  4.06823468208313\n","Epoch:  65  t_loss:  2942.6108  v_loss:  2959.3577  t:  4.02639102935791\n","Epoch:  66  t_loss:  2914.2087  v_loss:  2931.3025  t:  3.841925621032715\n","Epoch:  67  t_loss:  2885.6873  v_loss:  2903.1025  t:  3.932767629623413\n","Epoch:  68  t_loss:  2857.2197  v_loss:  2874.919  t:  3.9395813941955566\n","Epoch:  69  t_loss:  2828.983  v_loss:  2846.9185  t:  3.9265103340148926\n","Epoch:  70  t_loss:  2801.1506  v_loss:  2819.2676  t:  3.8252451419830322\n","Epoch:  71  t_loss:  2773.8865  v_loss:  2792.1265  t:  4.031397342681885\n","Epoch:  72  t_loss:  2747.3403  v_loss:  2765.6409  t:  3.8256330490112305\n","Epoch:  73  t_loss:  2721.6406  v_loss:  2739.941  t:  3.9323856830596924\n","Epoch:  74  t_loss:  2696.8955  v_loss:  2715.1367  t:  3.908222198486328\n","Epoch:  75  t_loss:  2673.1885  v_loss:  2691.3157  t:  3.8518121242523193\n","Epoch:  76  t_loss:  2650.5796  v_loss:  2668.5427  t:  3.881553888320923\n","Epoch:  77  t_loss:  2629.1062  v_loss:  2646.8608  t:  3.9850213527679443\n","Epoch:  78  t_loss:  2608.7827  v_loss:  2626.293  t:  4.030885219573975\n","Epoch:  79  t_loss:  2589.6077  v_loss:  2606.8416  t:  3.9724886417388916\n","Epoch:  80  t_loss:  2571.5605  v_loss:  2588.4944  t:  3.905310869216919\n","Epoch:  81  t_loss:  2554.6104  v_loss:  2571.225  t:  3.912233352661133\n","Epoch:  82  t_loss:  2538.7158  v_loss:  2554.9988  t:  3.7963106632232666\n","Epoch:  83  t_loss:  2523.829  v_loss:  2539.7722  t:  3.888847827911377\n","Epoch:  84  t_loss:  2509.8984  v_loss:  2525.4966  t:  3.8819785118103027\n","Epoch:  85  t_loss:  2496.8694  v_loss:  2512.122  t:  3.8887197971343994\n","Epoch:  86  t_loss:  2484.6885  v_loss:  2499.5964  t:  3.8824009895324707\n","Epoch:  87  t_loss:  2473.302  v_loss:  2487.8696  t:  3.9625351428985596\n","Epoch:  88  t_loss:  2462.659  v_loss:  2476.891  t:  3.905245780944824\n","Epoch:  89  t_loss:  2452.709  v_loss:  2466.6125  t:  3.8533456325531006\n","Epoch:  90  t_loss:  2443.4048  v_loss:  2456.9875  t:  3.8667232990264893\n","Epoch:  91  t_loss:  2434.7026  v_loss:  2447.9727  t:  3.915992259979248\n","Epoch:  92  t_loss:  2426.5593  v_loss:  2439.5261  t:  3.942070722579956\n","Epoch:  93  t_loss:  2418.9355  v_loss:  2431.6086  t:  3.9262492656707764\n","Epoch:  94  t_loss:  2411.794  v_loss:  2424.1826  t:  3.8565845489501953\n","Epoch:  95  t_loss:  2405.0996  v_loss:  2417.214  t:  3.862190008163452\n","Epoch:  96  t_loss:  2398.8206  v_loss:  2410.67  t:  3.909315586090088\n","Epoch:  97  t_loss:  2392.9255  v_loss:  2404.52  t:  4.0087971687316895\n","Epoch:  98  t_loss:  2387.3875  v_loss:  2398.7358  t:  3.8670663833618164\n","Epoch:  99  t_loss:  2382.18  v_loss:  2393.2913  t:  3.8674702644348145\n","Epoch:  100  t_loss:  2377.2786  v_loss:  2388.162  t:  3.864542007446289\n","Epoch:  101  t_loss:  2372.662  v_loss:  2383.3262  t:  3.852386236190796\n","Epoch:  102  t_loss:  2368.3093  v_loss:  2378.762  t:  3.836099863052368\n","Epoch:  103  t_loss:  2364.2014  v_loss:  2374.4507  t:  3.9275639057159424\n","Epoch:  104  t_loss:  2360.3208  v_loss:  2370.375  t:  3.8661909103393555\n","Epoch:  105  t_loss:  2356.6516  v_loss:  2366.5183  t:  3.997649908065796\n","Epoch:  106  t_loss:  2353.179  v_loss:  2362.8652  t:  3.868816375732422\n","Epoch:  107  t_loss:  2349.8892  v_loss:  2359.402  t:  3.882337808609009\n","Epoch:  108  t_loss:  2346.7695  v_loss:  2356.1162  t:  3.8669393062591553\n","Epoch:  109  t_loss:  2343.8086  v_loss:  2352.9956  t:  3.9296934604644775\n","Epoch:  110  t_loss:  2340.9956  v_loss:  2350.0288  t:  3.9477829933166504\n","Epoch:  111  t_loss:  2338.3208  v_loss:  2347.2068  t:  3.873039484024048\n","Epoch:  112  t_loss:  2335.775  v_loss:  2344.5195  t:  3.904102087020874\n","Epoch:  113  t_loss:  2333.349  v_loss:  2341.9583  t:  3.789289712905884\n","Epoch:  114  t_loss:  2331.0366  v_loss:  2339.5154  t:  3.935460090637207\n","Epoch:  115  t_loss:  2328.8298  v_loss:  2337.183  t:  3.8642430305480957\n","Epoch:  116  t_loss:  2326.722  v_loss:  2334.9553  t:  3.818934440612793\n","Epoch:  117  t_loss:  2324.7068  v_loss:  2332.8247  t:  3.8941452503204346\n","Epoch:  118  t_loss:  2322.7793  v_loss:  2330.7861  t:  3.8127453327178955\n","Epoch:  119  t_loss:  2320.9333  v_loss:  2328.8342  t:  3.8230838775634766\n","Epoch:  120  t_loss:  2319.1648  v_loss:  2326.9631  t:  3.931882858276367\n","Epoch:  121  t_loss:  2317.469  v_loss:  2325.169  t:  3.895186185836792\n","Epoch:  122  t_loss:  2315.8418  v_loss:  2323.4473  t:  3.8735666275024414\n","Epoch:  123  t_loss:  2314.2788  v_loss:  2321.7935  t:  3.930516004562378\n","Epoch:  124  t_loss:  2312.7769  v_loss:  2320.204  t:  3.9379143714904785\n","Epoch:  125  t_loss:  2311.3328  v_loss:  2318.6763  t:  3.8593273162841797\n","Epoch:  126  t_loss:  2309.9434  v_loss:  2317.2058  t:  3.9003474712371826\n","Epoch:  127  t_loss:  2308.6055  v_loss:  2315.79  t:  3.836155891418457\n","Epoch:  128  t_loss:  2307.3164  v_loss:  2314.4263  t:  3.8641083240509033\n","Epoch:  129  t_loss:  2306.0735  v_loss:  2313.1113  t:  3.8488974571228027\n","Epoch:  130  t_loss:  2304.875  v_loss:  2311.8435  t:  3.926262855529785\n","Epoch:  131  t_loss:  2303.7178  v_loss:  2310.6204  t:  3.8572609424591064\n","Epoch:  132  t_loss:  2302.6008  v_loss:  2309.439  t:  3.990647077560425\n","Epoch:  133  t_loss:  2301.5215  v_loss:  2308.2979  t:  3.8194425106048584\n","Epoch:  134  t_loss:  2300.4785  v_loss:  2307.1953  t:  3.946535110473633\n","Epoch:  135  t_loss:  2299.4692  v_loss:  2306.1287  t:  4.0057220458984375\n","Epoch:  136  t_loss:  2298.493  v_loss:  2305.0972  t:  3.848170757293701\n","Epoch:  137  t_loss:  2297.5476  v_loss:  2304.099  t:  3.8371706008911133\n","Epoch:  138  t_loss:  2296.6323  v_loss:  2303.133  t:  3.9535329341888428\n","Epoch:  139  t_loss:  2295.7456  v_loss:  2302.197  t:  3.9508564472198486\n","Epoch:  140  t_loss:  2294.886  v_loss:  2301.2896  t:  3.896556854248047\n","Epoch:  141  t_loss:  2294.053  v_loss:  2300.4106  t:  3.836026906967163\n","Epoch:  142  t_loss:  2293.2446  v_loss:  2299.558  t:  3.85538649559021\n","Epoch:  143  t_loss:  2292.46  v_loss:  2298.7312  t:  3.9367730617523193\n","Epoch:  144  t_loss:  2291.6987  v_loss:  2297.9287  t:  3.963883638381958\n","Epoch:  145  t_loss:  2290.9595  v_loss:  2297.1497  t:  3.9399099349975586\n","Epoch:  146  t_loss:  2290.2417  v_loss:  2296.3936  t:  3.860102891921997\n","Epoch:  147  t_loss:  2289.5437  v_loss:  2295.659  t:  3.9235944747924805\n","Epoch:  148  t_loss:  2288.8655  v_loss:  2294.9453  t:  3.8993372917175293\n","Epoch:  149  t_loss:  2288.2063  v_loss:  2294.2517  t:  3.847203016281128\n","Epoch:  150  t_loss:  2287.5652  v_loss:  2293.5771  t:  3.890244245529175\n","Epoch:  151  t_loss:  2286.942  v_loss:  2292.9214  t:  3.9076011180877686\n","Epoch:  152  t_loss:  2286.3347  v_loss:  2292.2837  t:  3.9063761234283447\n","Epoch:  153  t_loss:  2285.7444  v_loss:  2291.663  t:  3.9705402851104736\n","Epoch:  154  t_loss:  2285.1697  v_loss:  2291.0593  t:  3.9607362747192383\n","Epoch:  155  t_loss:  2284.6096  v_loss:  2290.4717  t:  3.925503969192505\n","Epoch:  156  t_loss:  2284.065  v_loss:  2289.8994  t:  3.880030870437622\n","Epoch:  157  t_loss:  2283.5337  v_loss:  2289.342  t:  3.935460329055786\n","Epoch:  158  t_loss:  2283.0164  v_loss:  2288.7993  t:  3.8570444583892822\n","Epoch:  159  t_loss:  2282.5122  v_loss:  2288.2705  t:  3.887150764465332\n","Epoch:  160  t_loss:  2282.0208  v_loss:  2287.7554  t:  3.962465524673462\n","Epoch:  161  t_loss:  2281.5415  v_loss:  2287.2534  t:  3.9285175800323486\n","Epoch:  162  t_loss:  2281.0747  v_loss:  2286.7642  t:  4.026630878448486\n","Epoch:  163  t_loss:  2280.6191  v_loss:  2286.2869  t:  3.893892288208008\n","Epoch:  164  t_loss:  2280.1748  v_loss:  2285.8218  t:  3.8523857593536377\n","Epoch:  165  t_loss:  2279.7417  v_loss:  2285.368  t:  3.879047155380249\n","Epoch:  166  t_loss:  2279.3184  v_loss:  2284.9255  t:  3.9052631855010986\n","Epoch:  167  t_loss:  2278.9062  v_loss:  2284.4934  t:  3.912982940673828\n","Epoch:  168  t_loss:  2278.5034  v_loss:  2284.0723  t:  3.9849393367767334\n","Epoch:  169  t_loss:  2278.1104  v_loss:  2283.6606  t:  3.9566080570220947\n","Epoch:  170  t_loss:  2277.7266  v_loss:  2283.2598  t:  4.01045036315918\n","Epoch:  171  t_loss:  2277.3518  v_loss:  2282.8677  t:  3.992889642715454\n","Epoch:  172  t_loss:  2276.986  v_loss:  2282.4854  t:  3.9908878803253174\n","Epoch:  173  t_loss:  2276.6287  v_loss:  2282.1118  t:  3.976440191268921\n","Epoch:  174  t_loss:  2276.2798  v_loss:  2281.747  t:  3.8325130939483643\n","Epoch:  175  t_loss:  2275.9387  v_loss:  2281.3909  t:  3.9108939170837402\n","Epoch:  176  t_loss:  2275.6057  v_loss:  2281.0427  t:  3.950904369354248\n","Epoch:  177  t_loss:  2275.2808  v_loss:  2280.7026  t:  3.9297492504119873\n","Epoch:  178  t_loss:  2274.9626  v_loss:  2280.3704  t:  3.983078718185425\n","Epoch:  179  t_loss:  2274.6519  v_loss:  2280.046  t:  3.87154221534729\n","Epoch:  180  t_loss:  2274.3481  v_loss:  2279.7288  t:  3.961543083190918\n","Epoch:  181  t_loss:  2274.0513  v_loss:  2279.4182  t:  3.975348711013794\n","Epoch:  182  t_loss:  2273.7612  v_loss:  2279.1152  t:  3.876148223876953\n","Epoch:  183  t_loss:  2273.4775  v_loss:  2278.8188  t:  3.890300989151001\n","Epoch:  184  t_loss:  2273.1997  v_loss:  2278.5286  t:  3.946246385574341\n","Epoch:  185  t_loss:  2272.9287  v_loss:  2278.245  t:  3.872436761856079\n","Epoch:  186  t_loss:  2272.6636  v_loss:  2277.968  t:  4.033444404602051\n","Epoch:  187  t_loss:  2272.404  v_loss:  2277.6968  t:  3.926175355911255\n","Epoch:  188  t_loss:  2272.1504  v_loss:  2277.4316  t:  3.8392703533172607\n","Epoch:  189  t_loss:  2271.9019  v_loss:  2277.1719  t:  3.9060513973236084\n","Epoch:  190  t_loss:  2271.6592  v_loss:  2276.918  t:  3.909709930419922\n","Epoch:  191  t_loss:  2271.4214  v_loss:  2276.6694  t:  3.9760968685150146\n","Epoch:  192  t_loss:  2271.1885  v_loss:  2276.4263  t:  3.8664824962615967\n","Epoch:  193  t_loss:  2270.961  v_loss:  2276.1882  t:  3.8316149711608887\n","Epoch:  194  t_loss:  2270.7385  v_loss:  2275.9548  t:  3.9577529430389404\n","Epoch:  195  t_loss:  2270.5203  v_loss:  2275.7266  t:  3.913168430328369\n","Epoch:  196  t_loss:  2270.3071  v_loss:  2275.5034  t:  3.906376600265503\n","Epoch:  197  t_loss:  2270.0981  v_loss:  2275.2844  t:  3.8978655338287354\n","Epoch:  198  t_loss:  2269.8938  v_loss:  2275.07  t:  3.9539620876312256\n","Epoch:  199  t_loss:  2269.6936  v_loss:  2274.86  t:  3.849959373474121\n","Epoch:  200  t_loss:  2269.4976  v_loss:  2274.6545  t:  3.9296679496765137\n","Epoch:  201  t_loss:  2269.3052  v_loss:  2274.4526  t:  3.815427541732788\n","Epoch:  202  t_loss:  2269.1174  v_loss:  2274.2554  t:  3.9656765460968018\n","Epoch:  203  t_loss:  2268.933  v_loss:  2274.062  t:  4.015767574310303\n","Epoch:  204  t_loss:  2268.7527  v_loss:  2273.872  t:  3.9571516513824463\n","Epoch:  205  t_loss:  2268.5757  v_loss:  2273.6865  t:  3.9766409397125244\n","Epoch:  206  t_loss:  2268.4026  v_loss:  2273.5042  t:  3.851715564727783\n","Epoch:  207  t_loss:  2268.233  v_loss:  2273.3254  t:  3.948840379714966\n","Epoch:  208  t_loss:  2268.0667  v_loss:  2273.1504  t:  3.9609341621398926\n","Epoch:  209  t_loss:  2267.9036  v_loss:  2272.9785  t:  3.894749879837036\n","Epoch:  210  t_loss:  2267.744  v_loss:  2272.8103  t:  3.9159152507781982\n","Epoch:  211  t_loss:  2267.5874  v_loss:  2272.645  t:  4.020219087600708\n","Epoch:  212  t_loss:  2267.4336  v_loss:  2272.4827  t:  3.969937801361084\n","Epoch:  213  t_loss:  2267.2832  v_loss:  2272.3235  t:  3.948674440383911\n","Epoch:  214  t_loss:  2267.1355  v_loss:  2272.1675  t:  3.911081314086914\n","Epoch:  215  t_loss:  2266.9912  v_loss:  2272.0146  t:  3.9614157676696777\n","Epoch:  216  t_loss:  2266.8489  v_loss:  2271.8643  t:  3.841146230697632\n","Epoch:  217  t_loss:  2266.7097  v_loss:  2271.7166  t:  3.8683853149414062\n","Epoch:  218  t_loss:  2266.5735  v_loss:  2271.5715  t:  3.916593313217163\n","Epoch:  219  t_loss:  2266.4395  v_loss:  2271.4297  t:  3.8557305335998535\n","Epoch:  220  t_loss:  2266.3079  v_loss:  2271.2898  t:  3.93314528465271\n","Epoch:  221  t_loss:  2266.1787  v_loss:  2271.1528  t:  3.8674893379211426\n","Epoch:  222  t_loss:  2266.0525  v_loss:  2271.018  t:  3.9851794242858887\n","Epoch:  223  t_loss:  2265.9282  v_loss:  2270.8857  t:  3.896822452545166\n","Epoch:  224  t_loss:  2265.8062  v_loss:  2270.7556  t:  3.91569447517395\n","Epoch:  225  t_loss:  2265.6865  v_loss:  2270.628  t:  3.9376938343048096\n","Epoch:  226  t_loss:  2265.5693  v_loss:  2270.5024  t:  4.043109178543091\n","Epoch:  227  t_loss:  2265.4539  v_loss:  2270.3792  t:  3.971557378768921\n","Epoch:  228  t_loss:  2265.3408  v_loss:  2270.2578  t:  3.9340898990631104\n","Epoch:  229  t_loss:  2265.2295  v_loss:  2270.1387  t:  3.9038898944854736\n","Epoch:  230  t_loss:  2265.12  v_loss:  2270.0215  t:  3.982043504714966\n","Epoch:  231  t_loss:  2265.013  v_loss:  2269.9062  t:  4.008630990982056\n","Epoch:  232  t_loss:  2264.9072  v_loss:  2269.7932  t:  3.9015963077545166\n","Epoch:  233  t_loss:  2264.8037  v_loss:  2269.6816  t:  3.8957176208496094\n","Epoch:  234  t_loss:  2264.7021  v_loss:  2269.5718  t:  3.93131685256958\n","Epoch:  235  t_loss:  2264.602  v_loss:  2269.4639  t:  3.974536418914795\n","Epoch:  236  t_loss:  2264.504  v_loss:  2269.358  t:  3.954169750213623\n","Epoch:  237  t_loss:  2264.4072  v_loss:  2269.2537  t:  3.916898488998413\n","Epoch:  238  t_loss:  2264.3123  v_loss:  2269.151  t:  3.9584293365478516\n","Epoch:  239  t_loss:  2264.219  v_loss:  2269.0498  t:  3.9175288677215576\n","Epoch:  240  t_loss:  2264.1274  v_loss:  2268.9502  t:  3.8919782638549805\n","Epoch:  241  t_loss:  2264.0369  v_loss:  2268.8518  t:  3.8621938228607178\n","Epoch:  242  t_loss:  2263.948  v_loss:  2268.7556  t:  3.9113516807556152\n","Epoch:  243  t_loss:  2263.8606  v_loss:  2268.6604  t:  3.850546360015869\n","Epoch:  244  t_loss:  2263.775  v_loss:  2268.567  t:  3.995786190032959\n","Epoch:  245  t_loss:  2263.6904  v_loss:  2268.4746  t:  3.901864767074585\n","Epoch:  246  t_loss:  2263.6072  v_loss:  2268.3838  t:  3.8835201263427734\n","Epoch:  247  t_loss:  2263.5256  v_loss:  2268.2944  t:  3.837104558944702\n","Epoch:  248  t_loss:  2263.445  v_loss:  2268.2063  t:  3.88218092918396\n","Epoch:  249  t_loss:  2263.3657  v_loss:  2268.1196  t:  3.928001880645752\n","Epoch:  250  t_loss:  2263.288  v_loss:  2268.034  t:  3.89725661277771\n","Epoch:  251  t_loss:  2263.2112  v_loss:  2267.9497  t:  3.8650453090667725\n","Epoch:  252  t_loss:  2263.1357  v_loss:  2267.8662  t:  3.957231044769287\n","Epoch:  253  t_loss:  2263.061  v_loss:  2267.7842  t:  3.9750466346740723\n","Epoch:  254  t_loss:  2262.9883  v_loss:  2267.7036  t:  3.9203133583068848\n","Epoch:  255  t_loss:  2262.9155  v_loss:  2267.6235  t:  3.9866108894348145\n","Epoch:  256  t_loss:  2262.8447  v_loss:  2267.5452  t:  4.03924298286438\n","Epoch:  257  t_loss:  2262.7747  v_loss:  2267.4675  t:  3.9687767028808594\n","Epoch:  258  t_loss:  2262.7056  v_loss:  2267.391  t:  4.068034887313843\n","Epoch:  259  t_loss:  2262.638  v_loss:  2267.3157  t:  3.9289495944976807\n","Epoch:  260  t_loss:  2262.5708  v_loss:  2267.2412  t:  3.9982831478118896\n","Epoch:  261  t_loss:  2262.5046  v_loss:  2267.168  t:  3.9810194969177246\n","Epoch:  262  t_loss:  2262.4395  v_loss:  2267.0955  t:  3.9514150619506836\n","Epoch:  263  t_loss:  2262.3755  v_loss:  2267.024  t:  4.039305925369263\n","Epoch:  264  t_loss:  2262.3123  v_loss:  2266.9534  t:  3.9559836387634277\n","Epoch:  265  t_loss:  2262.25  v_loss:  2266.8835  t:  3.999593496322632\n","Epoch:  266  t_loss:  2262.1887  v_loss:  2266.815  t:  4.041968107223511\n","Epoch:  267  t_loss:  2262.128  v_loss:  2266.747  t:  3.9834508895874023\n","Epoch:  268  t_loss:  2262.0684  v_loss:  2266.68  t:  4.086665630340576\n","Epoch:  269  t_loss:  2262.0093  v_loss:  2266.6138  t:  3.971815347671509\n","Epoch:  270  t_loss:  2261.9512  v_loss:  2266.5486  t:  3.9224417209625244\n","Epoch:  271  t_loss:  2261.8938  v_loss:  2266.484  t:  3.9059431552886963\n","Epoch:  272  t_loss:  2261.837  v_loss:  2266.4202  t:  4.008211374282837\n","Epoch:  273  t_loss:  2261.7812  v_loss:  2266.3572  t:  3.9774374961853027\n","Epoch:  274  t_loss:  2261.726  v_loss:  2266.2947  t:  3.9566664695739746\n","Epoch:  275  t_loss:  2261.6719  v_loss:  2266.2334  t:  3.9926562309265137\n","Epoch:  276  t_loss:  2261.618  v_loss:  2266.1726  t:  4.087909936904907\n","Epoch:  277  t_loss:  2261.565  v_loss:  2266.1128  t:  4.048793077468872\n","Epoch:  278  t_loss:  2261.5127  v_loss:  2266.0535  t:  4.0584046840667725\n","Epoch:  279  t_loss:  2261.4612  v_loss:  2265.9949  t:  4.032418251037598\n","Epoch:  280  t_loss:  2261.4102  v_loss:  2265.937  t:  4.057663917541504\n","Epoch:  281  t_loss:  2261.3599  v_loss:  2265.88  t:  4.116591691970825\n","Epoch:  282  t_loss:  2261.31  v_loss:  2265.8232  t:  4.091385126113892\n","Epoch:  283  t_loss:  2261.2607  v_loss:  2265.7676  t:  4.08367133140564\n","Epoch:  284  t_loss:  2261.2124  v_loss:  2265.712  t:  4.064455986022949\n","Epoch:  285  t_loss:  2261.1643  v_loss:  2265.6572  t:  4.1026177406311035\n","Epoch:  286  t_loss:  2261.1172  v_loss:  2265.6033  t:  4.0139923095703125\n","Epoch:  287  t_loss:  2261.0703  v_loss:  2265.5498  t:  4.032034158706665\n","Epoch:  288  t_loss:  2261.024  v_loss:  2265.497  t:  4.0907227993011475\n","Epoch:  289  t_loss:  2260.9785  v_loss:  2265.4446  t:  4.050605297088623\n","Epoch:  290  t_loss:  2260.933  v_loss:  2265.393  t:  4.002835512161255\n","Epoch:  291  t_loss:  2260.8887  v_loss:  2265.3418  t:  4.067218780517578\n","Epoch:  292  t_loss:  2260.8445  v_loss:  2265.2913  t:  4.07192587852478\n","Epoch:  293  t_loss:  2260.8008  v_loss:  2265.2412  t:  4.082570552825928\n","Epoch:  294  t_loss:  2260.7576  v_loss:  2265.1917  t:  4.117387771606445\n","Epoch:  295  t_loss:  2260.715  v_loss:  2265.1426  t:  4.024956226348877\n","Epoch:  296  t_loss:  2260.6729  v_loss:  2265.0942  t:  4.001787900924683\n","Epoch:  297  t_loss:  2260.631  v_loss:  2265.0466  t:  4.158855199813843\n","Epoch:  298  t_loss:  2260.5898  v_loss:  2264.9993  t:  4.097841262817383\n","Epoch:  299  t_loss:  2260.549  v_loss:  2264.9521  t:  4.0457775592803955\n","Epoch:  300  t_loss:  2260.5088  v_loss:  2264.9058  t:  4.05155611038208\n","Epoch:  301  t_loss:  2260.4688  v_loss:  2264.8596  t:  4.15237832069397\n","Epoch:  302  t_loss:  2260.4292  v_loss:  2264.8142  t:  3.9784042835235596\n","Epoch:  303  t_loss:  2260.3904  v_loss:  2264.7695  t:  4.089386701583862\n","Epoch:  304  t_loss:  2260.3516  v_loss:  2264.7249  t:  4.203431844711304\n","Epoch:  305  t_loss:  2260.3132  v_loss:  2264.6807  t:  3.973252534866333\n","Epoch:  306  t_loss:  2260.2754  v_loss:  2264.6372  t:  4.173208951950073\n","Epoch:  307  t_loss:  2260.2383  v_loss:  2264.5938  t:  4.137246608734131\n","Epoch:  308  t_loss:  2260.201  v_loss:  2264.551  t:  4.043498516082764\n","Epoch:  309  t_loss:  2260.1643  v_loss:  2264.5085  t:  4.049259662628174\n","Epoch:  310  t_loss:  2260.128  v_loss:  2264.467  t:  4.020295143127441\n","Epoch:  311  t_loss:  2260.0918  v_loss:  2264.4253  t:  3.97196626663208\n","Epoch:  312  t_loss:  2260.0564  v_loss:  2264.3843  t:  4.089789390563965\n","Epoch:  313  t_loss:  2260.0212  v_loss:  2264.3435  t:  4.0552356243133545\n","Epoch:  314  t_loss:  2259.986  v_loss:  2264.3032  t:  4.0827577114105225\n","Epoch:  315  t_loss:  2259.9517  v_loss:  2264.2632  t:  4.099944114685059\n","Epoch:  316  t_loss:  2259.9172  v_loss:  2264.2236  t:  4.035517692565918\n","Epoch:  317  t_loss:  2259.8835  v_loss:  2264.1846  t:  4.077256441116333\n","Epoch:  318  t_loss:  2259.85  v_loss:  2264.146  t:  4.060041189193726\n","Epoch:  319  t_loss:  2259.8164  v_loss:  2264.1074  t:  4.036881685256958\n","Epoch:  320  t_loss:  2259.7834  v_loss:  2264.0696  t:  3.968137502670288\n","Epoch:  321  t_loss:  2259.7507  v_loss:  2264.032  t:  4.000492095947266\n","Epoch:  322  t_loss:  2259.7185  v_loss:  2263.9941  t:  3.9739348888397217\n","Epoch:  323  t_loss:  2259.6865  v_loss:  2263.9575  t:  4.012261390686035\n","Epoch:  324  t_loss:  2259.6548  v_loss:  2263.921  t:  4.068587303161621\n","Epoch:  325  t_loss:  2259.6233  v_loss:  2263.8848  t:  4.014212608337402\n","Epoch:  326  t_loss:  2259.592  v_loss:  2263.8486  t:  4.008222579956055\n","Epoch:  327  t_loss:  2259.561  v_loss:  2263.813  t:  4.123239517211914\n","Epoch:  328  t_loss:  2259.5303  v_loss:  2263.7778  t:  4.010917663574219\n","Epoch:  329  t_loss:  2259.5  v_loss:  2263.743  t:  4.069819450378418\n","Epoch:  330  t_loss:  2259.47  v_loss:  2263.7085  t:  3.9712307453155518\n","Epoch:  331  t_loss:  2259.4404  v_loss:  2263.6738  t:  4.047042369842529\n","Epoch:  332  t_loss:  2259.4106  v_loss:  2263.64  t:  4.037353992462158\n","Epoch:  333  t_loss:  2259.381  v_loss:  2263.6062  t:  4.071752548217773\n","Epoch:  334  t_loss:  2259.352  v_loss:  2263.573  t:  4.115778923034668\n","Epoch:  335  t_loss:  2259.3232  v_loss:  2263.54  t:  4.03744101524353\n","Epoch:  336  t_loss:  2259.2944  v_loss:  2263.5068  t:  4.016521453857422\n","Epoch:  337  t_loss:  2259.266  v_loss:  2263.4746  t:  3.962278127670288\n","Epoch:  338  t_loss:  2259.238  v_loss:  2263.4424  t:  4.030685186386108\n","Epoch:  339  t_loss:  2259.2104  v_loss:  2263.4106  t:  4.147063970565796\n","Epoch:  340  t_loss:  2259.1824  v_loss:  2263.379  t:  4.08003044128418\n","Epoch:  341  t_loss:  2259.1548  v_loss:  2263.3477  t:  3.9698941707611084\n","Epoch:  342  t_loss:  2259.1277  v_loss:  2263.317  t:  4.056370496749878\n","Epoch:  343  t_loss:  2259.1003  v_loss:  2263.286  t:  4.075983047485352\n","Epoch:  344  t_loss:  2259.074  v_loss:  2263.2554  t:  4.053093671798706\n","Epoch:  345  t_loss:  2259.0474  v_loss:  2263.225  t:  3.986480236053467\n","Epoch:  346  t_loss:  2259.021  v_loss:  2263.195  t:  4.014403581619263\n","Epoch:  347  t_loss:  2258.9946  v_loss:  2263.1653  t:  4.055609464645386\n","Epoch:  348  t_loss:  2258.9685  v_loss:  2263.1357  t:  3.940263032913208\n","Epoch:  349  t_loss:  2258.9426  v_loss:  2263.1064  t:  3.943331480026245\n","Epoch:  350  t_loss:  2258.917  v_loss:  2263.078  t:  3.9916248321533203\n","Epoch:  351  t_loss:  2258.8916  v_loss:  2263.0493  t:  3.987086057662964\n","Epoch:  352  t_loss:  2258.8662  v_loss:  2263.0208  t:  4.02750563621521\n","Epoch:  353  t_loss:  2258.841  v_loss:  2262.9927  t:  4.0875563621521\n","Epoch:  354  t_loss:  2258.8164  v_loss:  2262.9646  t:  4.025279760360718\n","Epoch:  355  t_loss:  2258.7915  v_loss:  2262.9368  t:  4.013933181762695\n","Epoch:  356  t_loss:  2258.767  v_loss:  2262.9097  t:  3.986337184906006\n","Epoch:  357  t_loss:  2258.7427  v_loss:  2262.8823  t:  4.009943246841431\n","Epoch:  358  t_loss:  2258.7185  v_loss:  2262.8552  t:  3.9898736476898193\n","Epoch:  359  t_loss:  2258.6943  v_loss:  2262.8284  t:  4.0477588176727295\n","Epoch:  360  t_loss:  2258.6707  v_loss:  2262.8018  t:  3.9706006050109863\n","Epoch:  361  t_loss:  2258.647  v_loss:  2262.7754  t:  3.969273805618286\n","Epoch:  362  t_loss:  2258.6233  v_loss:  2262.7495  t:  3.965881109237671\n","Epoch:  363  t_loss:  2258.5999  v_loss:  2262.7236  t:  3.935053586959839\n","Epoch:  364  t_loss:  2258.5767  v_loss:  2262.698  t:  4.000848054885864\n","Epoch:  365  t_loss:  2258.5537  v_loss:  2262.6726  t:  3.9755146503448486\n","Epoch:  366  t_loss:  2258.5308  v_loss:  2262.6472  t:  4.00391411781311\n","Epoch:  367  t_loss:  2258.508  v_loss:  2262.6223  t:  3.939887523651123\n","Epoch:  368  t_loss:  2258.4854  v_loss:  2262.5974  t:  3.923070192337036\n","Epoch:  369  t_loss:  2258.463  v_loss:  2262.573  t:  4.014254570007324\n","Epoch:  370  t_loss:  2258.4407  v_loss:  2262.5483  t:  4.0137879848480225\n","Epoch:  371  t_loss:  2258.4185  v_loss:  2262.5244  t:  4.015672445297241\n","Epoch:  372  t_loss:  2258.3965  v_loss:  2262.5005  t:  4.03322434425354\n","Epoch:  373  t_loss:  2258.3745  v_loss:  2262.4766  t:  3.96553111076355\n","Epoch:  374  t_loss:  2258.3528  v_loss:  2262.453  t:  3.9791390895843506\n","Epoch:  375  t_loss:  2258.331  v_loss:  2262.4294  t:  4.0382773876190186\n","Epoch:  376  t_loss:  2258.3096  v_loss:  2262.4062  t:  4.069854259490967\n","Epoch:  377  t_loss:  2258.2886  v_loss:  2262.3833  t:  3.964961528778076\n","Epoch:  378  t_loss:  2258.267  v_loss:  2262.3604  t:  3.9734628200531006\n","Epoch:  379  t_loss:  2258.246  v_loss:  2262.3376  t:  4.054275751113892\n","Epoch:  380  t_loss:  2258.2249  v_loss:  2262.3154  t:  3.949205160140991\n","Epoch:  381  t_loss:  2258.2043  v_loss:  2262.2932  t:  4.009875535964966\n","Epoch:  382  t_loss:  2258.1836  v_loss:  2262.271  t:  3.9968676567077637\n","Epoch:  383  t_loss:  2258.1628  v_loss:  2262.249  t:  4.060408592224121\n","Epoch:  384  t_loss:  2258.1423  v_loss:  2262.2273  t:  3.9979023933410645\n","Epoch:  385  t_loss:  2258.1218  v_loss:  2262.2058  t:  3.993021249771118\n","Epoch:  386  t_loss:  2258.1016  v_loss:  2262.1843  t:  3.958369016647339\n","Epoch:  387  t_loss:  2258.0815  v_loss:  2262.1633  t:  3.937410593032837\n","Epoch:  388  t_loss:  2258.0613  v_loss:  2262.142  t:  3.9313061237335205\n","Epoch:  389  t_loss:  2258.0415  v_loss:  2262.121  t:  3.9379210472106934\n","Epoch:  390  t_loss:  2258.0215  v_loss:  2262.1006  t:  3.933454751968384\n","Epoch:  391  t_loss:  2258.0017  v_loss:  2262.0798  t:  4.023027420043945\n","Epoch:  392  t_loss:  2257.9822  v_loss:  2262.0596  t:  4.053302049636841\n","Epoch:  393  t_loss:  2257.9626  v_loss:  2262.0393  t:  3.9936940670013428\n","Epoch:  394  t_loss:  2257.943  v_loss:  2262.0193  t:  3.9968438148498535\n","Epoch:  395  t_loss:  2257.9236  v_loss:  2261.9995  t:  4.002327919006348\n","Epoch:  396  t_loss:  2257.9043  v_loss:  2261.9795  t:  3.996589422225952\n","Epoch:  397  t_loss:  2257.8855  v_loss:  2261.96  t:  4.005629777908325\n","Epoch:  398  t_loss:  2257.8665  v_loss:  2261.9404  t:  3.9563915729522705\n","Epoch:  399  t_loss:  2257.8474  v_loss:  2261.9211  t:  3.9654200077056885\n","Epoch:  400  t_loss:  2257.8286  v_loss:  2261.902  t:  3.9524800777435303\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hv_3gvjEkoFk","colab_type":"text"},"source":["### Exercise 6: LSTM RNN (2 points)\n","\n","Long-Short Term Memory (LSTM) units have more [powerful functionality](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), such as sele\n","\n","1. `lstm` should be an `LSTM` layer leading from the `n_input` inputs to the `n_hidden` hidden units.\n","1. `fc` should be a fully-connected (linear) layer leading from the hidden units (output of `lstm`) to the `n_out` output units. \n","1. The network does not make use of further activation functions. "]},{"cell_type":"markdown","metadata":{"id":"MHmb_pg_koFm","colab_type":"text"},"source":["### Solution 6"]},{"cell_type":"code","metadata":{"id":"5YxC1IU8koFm","colab_type":"code","colab":{}},"source":["class Lstm(nn.Module):\n","  def __init__(self, n_input, n_hidden, n_out):\n","    super(Lstm, self).__init__()\n","    self.fc      = nn.Linear(n_hidden,n_out)\n","    self.lstm    = nn.LSTM(input_size = n_input, hidden_size=n_hidden, batch_first=True)\n","\n","  def forward(self,x):\n","    x = self.lstm(x)\n","    x = self.fc(x[0])\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"_GquR7GzkoFp","colab_type":"text"},"source":["### Exercise 7: Setup and run (1 point)\n","\n","Try your `LSTM` model with the dynamic addition task. \n","\n","1. Define the model. `LSTM` should have **2 hidden units**.\n","1. Define the loss as the Mean Squared Error loss, and use an Adam optimizer.\n","1. Train your model for several epochs on the data with `train_model`.\n","1. Plot the train and validation losses. \n","1. Test the trained model with `test_model` \n","1. Plot at least one target time series together with the predicted time series\n","\n","Did the LSTM network capture the task better? Did any of the two capture the task perfectly? Or are the two networks on par? "]},{"cell_type":"markdown","metadata":{"id":"dY2IrP2JkoFp","colab_type":"text"},"source":["### Solution 7"]},{"cell_type":"code","metadata":{"id":"yopvOgkmkoFp","colab_type":"code","colab":{}},"source":["lstm = Lstm(2,2,1)\n","\n","criterion = nn.MSELoss(reduction='sum')\n","optimizer = optim.Adam(lstm.parameters(),lr=0.006, amsgrad=False)\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = \"cpu\"\n","elman.to(device)\n","\n","model, train_loss, val_loss = train_model(lstm, data, optimizer, criterion, n_epochs=1000)\n","\n","# Plot losses\n","plt.plot(range(1,train_loss.shape[0]+1),train_loss)\n","plt.plot(range(1,val_loss.shape[0]+1),val_loss)\n","plt.legend(('train_loss', 'val_loss'), loc='upper right')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.title('Loss network')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PWzyO501koFr","colab_type":"code","colab":{}},"source":["predicted, targets = test_model(model, data)\n","\n","plt.figure(figsize=([20,10]))\n","plt.plot(range(0,3000),targets[1][0])\n","plt.plot(range(0,3000),predicted[1][0])\n","plt.legend(('target', 'prediction'), loc='upper right')\n","plt.ylabel('Value')\n","plt.xlabel('Time')\n","plt.title('Time Series')\n","plt.show()\n","\n","plt.figure(figsize=([20,10]))\n","plt.plot(range(0,3000),targets[2][0])\n","plt.plot(range(0,3000),predicted[2][0])\n","plt.legend(('target', 'prediction'), loc='upper right')\n","plt.ylabel('Value')\n","plt.xlabel('Time')\n","plt.title('Time Series')\n","plt.show()\n","\n","plt.figure(figsize=([20,10]))\n","plt.plot(range(0,3000),targets[8][0])\n","plt.plot(range(0,3000),predicted[8][0])\n","plt.legend(('target', 'prediction'), loc='upper right')\n","plt.ylabel('Value')\n","plt.xlabel('Time')\n","plt.title('Time Series')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpCbDQaZ1J4O","colab_type":"text"},"source":["Did the LSTM network capture the task better? Did any of the two capture the task perfectly? Or are the two networks on par? \n","\n","\n","1.   The LSTM network did capture the task better. We can see this in the graphs we create and we can see it in the loss.\n","2.   None of the two networks captured the task perfectly. In essence both networks tried to approximate the actual function as closely as possible. However it seems weird that the result is something like 2+2=3.99 which seems unintuitive.\n","3.  The two networks are not on par in the sense that the predictions of the lstm network were much closer to the actual values when compared to the elman network.\n"]}]}